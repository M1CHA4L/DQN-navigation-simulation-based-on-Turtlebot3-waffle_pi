# TurtleBot3 DQN Reinforcement Learning

## ğŸš€ Quick Start

### **Option 1: Training with Live Visualization** â­ RECOMMENDED
```bash
./run_with_visualization.sh
```
Opens: **Gazebo GUI** + **Live Graphs** (reward/loss/epsilon/steps) + **Training**

### **Option 2: Headless Training** (Faster)
```bash
./run_training.sh
```
Training only, no GUI - for faster performance

### **Option 3: Test Trained Model** ğŸ¯ NEW!
```bash
./run_testing.sh
```
Test your trained model and see performance statistics (requires trained model)

### **Option 4: Test Environment First**
```bash
python3 test_environment.py
```
Verify everything works before training (recommended first time)

---

## ğŸ§ª Testing Your Trained Model

After training, test your model's performance:

```bash
./run_testing.sh
```

### What the Test Script Does:
1. **Loads trained model** from `tb3_dqn_models_pytorch/dqn_model.pth`
2. **Runs multiple episodes** (you choose how many, default: 10)
3. **No exploration** - uses only learned policy (no random actions)
4. **Shows statistics**:
   - Success rate (% episodes reaching goal)
   - Average reward
   - Average steps per episode
   - Detailed results for each episode

### Sample Test Output:
```
Episode 1
Goal: (1.23, -0.87)
âœ… Episode 1 RESULT: REACHED goal in 142 steps!
   Total Reward: 456.32

Episode 2
Goal: (-1.45, 0.92)
âŒ Episode 2 RESULT: DID NOT reach goal (final dist: 0.35m, steps: 500)
   Total Reward: -42.18

TEST SUMMARY
Total Episodes: 10
Successful: 7 (70.0%)
Failed: 3 (30.0%)
Average Reward: 234.56
Average Steps: 287.3
Average Steps (successful episodes): 201.5
```

---

## ğŸ“Š What You'll See

**Terminal Output:**
```
Ep   1/200 | Steps: 125 | Reward:  -28.45 | Loss: 0.000 | Îµ: 0.995 | Mem: 125
Ep   1 RESULT: DID NOT reach goal (final dist 1.45m)
Ep   2/200 | Steps:  98 | Reward:  -22.31 | Loss: 0.156 | Îµ: 0.990 | Mem: 223
  â†’ New best: -22.31 (saved)
Ep   2 RESULT: REACHED goal at (-0.52,1.23)
```

**Episode Outcome Messages:**
- After each episode, you'll see whether the robot reached the goal or not
- **REACHED goal at (x,y)** - Robot successfully navigated to target! ğŸ‰
- **DID NOT reach goal (final dist Xm)** - Robot failed to reach target

**Live Graphs Window:** 4 plots updating every 2 seconds
- ğŸ“ˆ **Episode Reward** (blue) - trending up â†—ï¸
- ğŸ“‰ **Training Loss** (red) - decreasing â†˜ï¸
- ğŸ¯ **Epsilon Decay** (green) - 1.0 â†’ 0.01
- ğŸ“Š **Steps per Episode** (orange bars) - increasing

**Gazebo Window:** 
- Watch robot learn to navigate and avoid obstacles in real-time
- ğŸ¯ **Red goal box** visible at goal position (actual SDF model, not marker!)
- Goal position changes randomly each episode for better generalization
- Uses official ROBOTIS goal_box model from turtlebot3_gazebo package

---

## ğŸ“ Project Structure

```
ros2_rl_project/
â”œâ”€â”€ train_pytorch.py                    # Main DQN training script
â”œâ”€â”€ turtlebot_env_ros2.py               # Gymnasium RL environment
â”œâ”€â”€ test_trained_model.py               # ğŸ†• Test trained model
â”œâ”€â”€ visualize_training.py               # Live matplotlib graphs
â”œâ”€â”€ test_environment.py                 # Verification script
â”œâ”€â”€ run_training.sh                     # Headless launcher
â”œâ”€â”€ run_with_visualization.sh           # GUI + graphs launcher
â”œâ”€â”€ run_testing.sh                      # ğŸ†• Model testing launcher
â”œâ”€â”€ launch/                             # ROS2 launch files
â”‚   â”œâ”€â”€ turtlebot3_dqn_stage1_headless.launch.py
â”‚   â””â”€â”€ turtlebot3_dqn_stage1_with_gui.launch.py
â”œâ”€â”€ tb3_dqn_models_pytorch/             # Saved models
â””â”€â”€ turtlebot3_dqn_stage1_modified.world
```

---

## ğŸ“ For Your Report

### Screenshots to Take:
1. **Training graphs** after 200 episodes (all 4 plots)
2. **Gazebo** showing robot navigating successfully  
3. **Before/after comparison** (Episode 10 vs Episode 190)

### Graph Interpretation:
- **Reward**: Should improve from -150 to -25 (more positive = better navigation)
- **Loss**: Stabilizes around 0.15 (indicates learning convergence)
- **Epsilon**: Decays from 1.0 to 0.01 (exploration â†’ exploitation transition)
- **Steps**: Increases over time (robot survives longer, navigates better)

---

## âš™ï¸ Technical Details

- **Framework**: ROS2 Jazzy + Gazebo Harmonic + PyTorch
- **Algorithm**: DQN with experience replay and target network
- **State Space**: 22 dimensions (20 LiDAR + distance to goal + angle to goal)
- **Action Space**: 3 discrete actions (forward, turn left, turn right)
- **Training**: 200 episodes, max 500 steps per episode
- **Neural Network**: 3 hidden layers (512 â†’ 256 â†’ 64 neurons)
- **Replay Buffer**: 50,000 transitions
- **Epsilon Decay**: 0.99 per episode (1.0 â†’ 0.01)
- **Goal Generation**: Random positions in [-1.8m, 1.8m] square, avoiding obstacles
- **Goal Visualization**: Official ROBOTIS goal_box model (red cylinder entity)

---

## ğŸ”§ Troubleshooting

### If Training Seems Broken:

**Stop everything and restart:**
```bash
# Kill all processes
pkill -9 gz
pkill -9 python3
sleep 2

# Restart training
./run_with_visualization.sh
# OR
./run_training.sh
```

---

## ğŸ’¡ Important Notes

- âš ï¸ **Gazebo must start BEFORE training** (15-20 second wait required)
- ğŸ“ Training logs: `/tmp/training_log.txt`
- ğŸ’¾ Best model: Auto-saved to `tb3_dqn_models_pytorch/dqn_model.pth`
- ğŸ›‘ Stop training/testing: Press `Ctrl+C` (saves automatically)
- ğŸ¯ **Reset sequence**: Goal spawned â†’ Robot reset (prevents conflicts)

---

## ğŸ“ Quick Commands

```bash
# Train with visualization (EASIEST)
./run_with_visualization.sh

# Train headless (faster)
./run_training.sh

# Test trained model
./run_testing.sh

# Test environment first
python3 test_environment.py

# View training log
tail -f /tmp/training_log.txt

# Show graphs only
python3 visualize_training.py
```

